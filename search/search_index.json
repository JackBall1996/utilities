{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The site is built using MkDocs and the Material for MkDocs themes</p>"},{"location":"src-docs/","title":"Auto-generated Docs","text":"<p>Downloads a CSV file from an S3 bucket and loads it into a Pandas DataFrame.</p> <p>This op requires an S3 resource configured in the Dagster context. It reads the object specified by the <code>bucket</code> and <code>key</code> configuration fields, decodes it as UTF-8, and parses it into a Pandas DataFrame.</p> Config <p>bucket (str): Name of the S3 bucket. key (str): Key (path) to the CSV object within the S3 bucket.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The loaded DataFrame from the CSV file.</p> Source code in <code>Python/dagster/operators/operators.py</code> <pre><code>@op(\n    required_resource_keys={\"s3\"},\n    out=Out(pd.DataFrame),\n    config_schema={\"bucket\": String, \"key\": String},\n)\ndef read_csv_from_s3(context) -&gt; pd.DataFrame:\n    \"\"\"\n    Downloads a CSV file from an S3 bucket and loads it into a Pandas DataFrame.\n\n    This op requires an S3 resource configured in the Dagster context. It reads the\n    object specified by the `bucket` and `key` configuration fields, decodes it as UTF-8,\n    and parses it into a Pandas DataFrame.\n\n    Config:\n        bucket (str): Name of the S3 bucket.\n        key (str): Key (path) to the CSV object within the S3 bucket.\n\n    Returns:\n        pd.DataFrame: The loaded DataFrame from the CSV file.\n    \"\"\"\n    bucket = context.op_config[\"bucket\"]\n    key = context.op_config[\"key\"]\n\n    s3_client = context.resources.s3\n\n    context.log.info(f\"Downloading s3://{bucket}/{key}\")\n\n    response = s3_client.get_object(Bucket=bucket, Key=key)\n    csv_data = response[\"Body\"].read().decode(\"utf-8\")\n\n    df = pd.read_csv(StringIO(csv_data))\n    context.log.info(\n        f\"Loaded DataFrame with {len(df)} rows and {len(df.columns)} columns\"\n    )\n\n    return df\n</code></pre> <p>Writes a Pandas DataFrame to a CSV file and uploads it to an S3 bucket.</p> <p>This op converts the input DataFrame to a CSV string and uploads it to the specified S3 location using the configured S3 resource.</p> Config <p>bucket (str): Name of the destination S3 bucket. key (str): Key (path) within the bucket to store the CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>OpExecutionContext</code> <p>Dagster context containing configuration and resources.</p> required <code>df</code> <code>DataFrame</code> <p>DataFrame to serialize and upload to S3.</p> required Source code in <code>Python/dagster/operators/operators.py</code> <pre><code>@op(\n    ins={\"df\": In(pd.DataFrame)},\n    out=Out(Nothing),\n    required_resource_keys={\"s3\"},\n    config_schema={\"bucket\": String, \"key\": String},\n)\ndef write_csv_to_s3(context, df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Writes a Pandas DataFrame to a CSV file and uploads it to an S3 bucket.\n\n    This op converts the input DataFrame to a CSV string and uploads it to the specified\n    S3 location using the configured S3 resource.\n\n    Config:\n        bucket (str): Name of the destination S3 bucket.\n        key (str): Key (path) within the bucket to store the CSV file.\n\n    Args:\n        context (OpExecutionContext): Dagster context containing configuration and resources.\n        df (pd.DataFrame): DataFrame to serialize and upload to S3.\n    \"\"\"\n\n    bucket = context.op_config[\"bucket\"]\n    key = context.op_config[\"key\"]\n\n    context.log.info(f\"Writing DataFrame to s3://{bucket}/{key}\")\n\n    # Convert DataFrame to CSV in memory\n    csv_buffer = StringIO()\n    df.to_csv(csv_buffer, index=False)\n    csv_buffer.seek(0)\n\n    # Upload to S3 using the injected boto3 S3 client\n    s3_client = context.resources.s3\n    context.log.info(f\"{s3_client=}\")\n    s3_client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n\n    context.log.info(\"Upload Completed\")\n</code></pre> <p>Adds a column with the current timestamp to the input DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame to augment with a timestamp column.</p> required <code>timestamp_column_name</code> <code>str</code> <p>Name of the new column</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the added timestamp column.</p> Source code in <code>Python/functionality/transformations.py</code> <pre><code>def add_timestamp_column(df: pd.DataFrame, timestamp_column_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Adds a column with the current timestamp to the input DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame to augment with a timestamp column.\n        timestamp_column_name (str): Name of the new column\n\n    Returns:\n        pd.DataFrame: DataFrame with the added timestamp column.\n    \"\"\"\n    now = datetime.now()\n    df[timestamp_column_name] = now\n\n    return df\n</code></pre>"}]}